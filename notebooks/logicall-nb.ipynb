{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e94e9401",
   "metadata": {},
   "source": [
    "# üöÄ Logicall Project: Automated Shipment Tracking with YOLOv8\n",
    "\n",
    "## üìù Introduction\n",
    "This notebook provides a structured, step-by-step workflow for training a YOLOv8 model to address real-world challenges in logistics automation. By following this guide, users will gain a comprehensive understanding of setting up, training, and evaluating a state-of-the-art object detection model. Key performance metrics and visualizations are included to enhance the interpretability and impact of the results.\n",
    "\n",
    "## üì¶ Project Context: Logicall Shipment Tracking\n",
    "In the rapidly evolving logistics industry, efficient shipment tracking plays a crucial role in ensuring operational success and customer satisfaction. Logicall, a leading logistics provider, currently faces significant challenges due to manual shipment tracking processes. These challenges include:  \n",
    "\n",
    "- **Time inefficiencies:** Manual processes are prone to delays, particularly during peak shipment periods.  \n",
    "- **Human errors:** Dependency on manual labor increases the likelihood of misrecorded data or missed shipments.  \n",
    "- **Scalability issues:** As shipment volumes grow, the current system struggles to scale effectively, impacting overall efficiency and customer experience.  \n",
    "\n",
    "To address these challenges, Logicall seeks to develop an AI-powered automated tracking system that combines computer vision and spatial inference techniques.  \n",
    "\n",
    "## üéØ Key Objectives\n",
    "This project is designed to achieve the following objectives:  \n",
    "1. **Automate Barcode Detection:**  \n",
    "   Use the YOLOv8 object detection model to identify and extract barcodes from shipment packages efficiently, reducing manual intervention.  \n",
    "\n",
    "2. **Enhance Tracking Accuracy:**  \n",
    "   Develop a robust object tracking system to maintain consistent identification of individual shipments throughout the logistics pipeline.  \n",
    "\n",
    "3. **Implement Spatial Inference for Shipment Zoning (Future Scope):**  \n",
    "   Utilize spatial inference techniques to categorize shipment packages into predefined zones (e.g., staging, transit, delivery) for real-time shipment monitoring.  \n",
    "\n",
    "## üìñ Approach Overview\n",
    "The project workflow consists of the following phases:  \n",
    "1. **Dataset Preparation:**  \n",
    "   - Collection of high-quality annotated images containing shipment barcodes.  \n",
    "   - Dataset pre-processing to ensure compatibility with the YOLOv8 model requirements.  \n",
    "\n",
    "2. **Model Training:**  \n",
    "   - Configuring the YOLOv8 model with appropriate hyperparameters to maximize detection accuracy.  \n",
    "   - Iterative training with validation to optimize performance.  \n",
    "\n",
    "3. **Evaluation and Analysis:**  \n",
    "   - Quantitative evaluation using metrics such as mAP (mean Average Precision) and F1-score.  \n",
    "   - Qualitative analysis through visualizations of detection outputs and error cases.  \n",
    "\n",
    "4. **Deployment (Optional):**  \n",
    "   - Integration of the trained YOLOv8 model into Logicall's logistics pipeline.  \n",
    "   - Development of a dashboard or API for real-time shipment tracking.\n",
    "\n",
    "## üåü Expected Outcomes\n",
    "The successful implementation of this project will result in:  \n",
    "- A significant reduction in manual workload, enabling Logicall's team to focus on higher-value tasks.  \n",
    "- Improved tracking accuracy, minimizing errors and enhancing customer trust.  \n",
    "- Scalable solutions that can adapt to increasing shipment volumes seamlessly.  \n",
    "\n",
    "Through this project, Logicall aims to set a benchmark in logistics automation, leveraging cutting-edge AI technologies to stay ahead in the competitive landscape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77070e0",
   "metadata": {},
   "source": [
    "## üìö Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b78addfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "from ultralytics import YOLO\n",
    "from pyzbar.pyzbar import decode\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399be703",
   "metadata": {},
   "source": [
    "This following code organizes a dataset for training and validation in a YOLOv8 project. It starts by defining the directory paths for training and validation images and labels. It then ensures that the validation directories exist, creating them if necessary. Next, it retrieves a list of all image files in the training images directory and splits them into training and validation sets using an 80-20 ratio. The corresponding images and their associated label files (text files with annotations) for the validation set are moved to their respective validation directories. Finally, a message confirms that the validation dataset has been successfully created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dae10c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset created successfully.\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../data/\"\n",
    "images_dir = os.path.join(data_dir, \"train/images\")\n",
    "labels_dir = os.path.join(data_dir, \"train/labels\")\n",
    "valid_images_dir = os.path.join(data_dir, \"valid/images\")\n",
    "valid_labels_dir = os.path.join(data_dir, \"valid/labels\")\n",
    "\n",
    "os.makedirs(valid_images_dir, exist_ok=True)\n",
    "os.makedirs(valid_labels_dir, exist_ok=True)\n",
    "\n",
    "image_files = [f for f in os.listdir(images_dir) if f.endswith(\".jpg\")]\n",
    "\n",
    "train_images, valid_images = train_test_split(image_files, test_size=0.2, random_state=42)\n",
    "\n",
    "for img_file in valid_images:\n",
    "    shutil.move(os.path.join(images_dir, img_file), os.path.join(valid_images_dir, img_file))\n",
    "    label_file = img_file.replace(\".jpg\", \".txt\")\n",
    "    shutil.move(os.path.join(labels_dir, label_file), os.path.join(valid_labels_dir, label_file))\n",
    "\n",
    "print(\"Validation dataset created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8f2b89",
   "metadata": {},
   "source": [
    "This next code block initializes and trains a YOLOv8 model for object detection. It begins by importing the YOLO class from the Ultralytics library and loading a pre-trained YOLOv8n model, which serves as the starting point for training. The train method is then called to fine-tune the model using a custom dataset specified in the data.yaml file. Key training parameters include 50 epochs, an image size of 640 pixels and a batch size of 16. These settings determine the training duration, input image resolution and the number of images processed in each training batch, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f94f265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.146 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.144  Python-3.10.6 torch-2.7.0+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=../data/data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train4, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\detect\\train4, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751702  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,238 parameters, 3,011,222 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 288.679.9 MB/s, size: 54.2 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\Kaloyan\\Documents\\GitHub\\product-tracking\\data\\train\\labels... 326 images, 3 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 326/326 [00:00<00:00, 1127.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\Kaloyan\\Documents\\GitHub\\product-tracking\\data\\train\\labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 514.5110.3 MB/s, size: 51.5 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\Kaloyan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Kaloyan\\Documents\\GitHub\\product-tracking\\data\\valid\\labels... 184 images, 1 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 184/184 [00:00<00:00, 1534.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\Kaloyan\\Documents\\GitHub\\product-tracking\\data\\valid\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\Kaloyan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train4\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train4\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50         0G      1.829      3.856      1.369        253        640:  19%|‚ñà‚ñâ        | 4/21 [00:26<01:51,  6.55s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolov8n.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/data.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Kaloyan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\engine\\model.py:796\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[1;32m--> 796\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[0;32m    798\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[1;32mc:\\Users\\Kaloyan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\engine\\trainer.py:227\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    224\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 227\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Kaloyan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\engine\\trainer.py:415\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[1;34m(self, world_size)\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    411\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m*\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items) \u001b[38;5;241m/\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items\n\u001b[0;32m    412\u001b[0m     )\n\u001b[0;32m    414\u001b[0m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[1;32m--> 415\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;66;03m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ni \u001b[38;5;241m-\u001b[39m last_opt_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulate:\n",
      "File \u001b[1;32mc:\\Users\\Kaloyan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Kaloyan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Kaloyan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    " \n",
    "model = YOLO(\"yolov8n.pt\") \n",
    " \n",
    "model.train(\n",
    "    data=\"../data/data.yaml\",\n",
    "    epochs=50,\n",
    "    imgsz=640,\n",
    "    batch=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334f0721",
   "metadata": {},
   "source": [
    "# Model Evaluation (Company Provided Data Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd421f3",
   "metadata": {},
   "source": [
    "The model shows consistent improvement over the course of training. Loss values for bounding boxes, classification, and distribution focal loss steadily decrease, indicating that the model is learning effectively. Precision, recall, and mAP metrics improve as well, demonstrating better detection performance with more epochs and data. Early results start modestly, but as training continues, the model achieves higher accuracy and fewer false positives. GPU memory usage remains low and stable throughout training. Overall, the model performs reliably and shows clear progress as it trains on increasing amounts of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468e1549",
   "metadata": {},
   "source": [
    "# Model Evaluation (Data, collected on last Logicall Trip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9741548d",
   "metadata": {},
   "source": [
    "This updated model performs noticeably better than the previous one, largely thanks to being trained on a larger dataset. With more examples to learn from, the model picks up patterns faster and more accurately. The loss values‚Äîcovering bounding boxes, classification, and focal loss‚Äîdrop more smoothly and settle lower than before, showing that the model is learning efficiently. We also see solid improvements in key metrics like precision, recall, and mAP, meaning it‚Äôs doing a better job of correctly identifying objects and reducing mistakes. Compared to earlier results, it's more accurate and consistent, even in trickier cases. Despite training on more data, GPU usage stayed stable, which is a nice bonus. Overall, adding more data has made the model stronger, smarter, and more reliable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ae002a",
   "metadata": {},
   "source": [
    "## üéØ Barcode Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae8f9270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'x' to exit the barcode scanner.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m     cap\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m     41\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[1;32m---> 43\u001b[0m \u001b[43mscan_barcode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 13\u001b[0m, in \u001b[0;36mscan_barcode\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: Failed to capture frame.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m barcodes \u001b[38;5;241m=\u001b[39m \u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m barcode \u001b[38;5;129;01min\u001b[39;00m barcodes:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# extract barcode data\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     barcode_data \u001b[38;5;241m=\u001b[39m barcode\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Kaloyan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyzbar\\pyzbar.py:230\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(image, symbols)\u001b[0m\n\u001b[0;32m    228\u001b[0m zbar_image_set_size(img, width, height)\n\u001b[0;32m    229\u001b[0m zbar_image_set_data(img, cast(pixels, c_void_p), \u001b[38;5;28mlen\u001b[39m(pixels), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 230\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[43mzbar_scan_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscanner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoded \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PyZbarError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnsupported image format\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def scan_barcode():\n",
    "    # webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    scanned_barcodes = set()\n",
    "    print(\"Press 'x' to exit the barcode scanner.\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Failed to capture frame.\")\n",
    "            break\n",
    "\n",
    "        barcodes = decode(frame)\n",
    "        for barcode in barcodes:\n",
    "            # extract barcode data\n",
    "            barcode_data = barcode.data.decode('utf-8')\n",
    "\n",
    "            # skip if the barcode has already been scanned\n",
    "            if barcode_data in scanned_barcodes:\n",
    "                continue\n",
    "\n",
    "            # add new barcode to the set\n",
    "            scanned_barcodes.add(barcode_data)\n",
    "\n",
    "            # draw a rectangle around the detected barcode\n",
    "            (x, y, w, h) = barcode.rect\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "            text = f\"{barcode_data}\"\n",
    "            cv2.putText(frame, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "            print(f\"Barcode detected: {barcode_data}\")\n",
    "\n",
    "        cv2.imshow(\"Barcode Scanner\", frame)\n",
    "\n",
    "        # break the loop when 'x' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('x'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "scan_barcode()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
